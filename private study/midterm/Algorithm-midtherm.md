# ⭐1장⭐

- `알고리즘` : 문제 해결 절차를 체계적으로 기술한 것

    - 시간이나 공간의 제약이 없다

    - 명확해야한다 / 효율적이어야 한다

    - 알고리즘 ≠ 프로그램

    - 입력이 큰 경우에 대한 알고리즘 분석 → 점근적 분석(lim)

        ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/83115bcf-fcf1-4d62-b3fc-213eb010e73e/Untitled.png)

- `프로그램` : 알고리즘을 프로그래밍 언어로 기술한 것

- `알고리즘과 프로그램의 차이` : 알고리즘은 어떤 문제에 대한 해결 절차를 체계적으로 기술한 것이며, 프로그램은 해당 알고리즘을 토대로 프로그래밍 언어를 이용하여 작성한 것이다. 또한 알고리즘은 시간과 공간의 제약에서 자유롭지만 프로그램은 자유롭지 못하다.

- `알고리즘을 수행시간의 관점에서 분석하는 이유` : 알고리즘을 분석할 때는 통상 시간복잡도와 공간복잡도 2가지로 분석한다. 이때 시간과 공간은 반비례적 경향이 있기 때문에 통상 둘 다를 만족시키기엔 어렵다. 최근 대용량 시스템이 보편화 되면서 공간 복잡도 보다는 시간 복잡도가 우선시 되는 경향이 강하여 알고리즘은 주로 시간복잡도(수행시간의 관점)에서 분석한다.

- `알고리즘 수행시간의 의미` : 입력의 크기에 대해 수행시간이 어떤 비율로 소요 되는지에 대해 분석한 것

- `수행시간을 계산할 때 왜 특정한 명령문 수행분석으로 하는지?` : 알고리즘에서 입력의 크기가 작을 땐 각종 요소들이 수행시간에 직접적인 영향을 끼치지만, 입력의 크기가 충분히 커지면 반복문, 재귀호출과 같은 입력에 비례해 수행횟수가 증가하는 명령들이 알고리즘의 수행시간을 지배하게 된다. 때문에 보통 알고리즘의 수행시간은 반복문, 재귀호출과 같은 명령문이 수행되는 횟수로 측정되는 것이 일반적이다.

- 재귀호출(recursion) : 어떤 문제 안에 크기만 다를 뿐 성격이 똑같은 작은 문제가 포함되어 있는 것

- `점근적 분석이란?` : 알고리즘은 입력 크기가 아주 작으면 알고리즘의 효율성에 상관없이 금방 끝난다. 입력의 크기가 충분히 클 때 효율성이 중요해진다. 따라서 알고리즘의 수행시간을 분석할 때는 항상 입력의 크기가 충분히 클 때에 대해서 분석하는데 이를 점근적 분석이라 한다.

- `점근적 표기란?` : 변수의 크기가 충분히 큰 경우에 변수가 커짐에 따라 함수가 증가하는 비율을 표현하는 방법을 점근적 증가율이라 하고 그 표기법을 점근적 표기법이라 한다.

- **점근적 표기법**(3가지)(`Θ(f(n)) = O(f(n)) **∩** Ω(f(n))`)(식 순서에 유의하자)

    - Θ-표기법

        (증가율의 비례)

         : Θ(f(n))은 최고차항의 차수가 f(n)과 일치하는 함수의 집합이다.

        - n^2+5 = Θ(n^2) (O) / Θ(n^2) = n^2+5 (X) (why : =은 ∈대신 사용하는 기호이기 때문이다)

    - O-표기법

        (점근적 상한) : O(f(n))은 최고차항의 차수가 f(n)과 일치하거나 더 작은 함수의 집합이다.

        - n^2+5 = O(n^2) / 5n = O(n^2) / 500(상수) = O(n^2)

    - Ω-표기법(점근적 하한) : Ω(f(n))은 최고차항의 차수가 f(n)과 일치하거나 더 큰 함수의 집합이다.

        - n^2+5 = Ω(n^2) / 5n = Ω(n^2) / 500 = Ω(n^2)

- 시간복잡도 분석의 종류

    - worst-case
    - average-case
    - best-case(유용X)

------

# ⭐2장⭐

- 점화식이란?

    : 어떤 함수를 자신보다 더 작은 변수에 대한 함수와의 관계로 표현한 것

    - 재귀호출로 구성된 함수의 시간복잡도를 구하는데 주로 사용된다.

- 점화식의 점근적 분석(3가지)

    - 반복 대치 : 더 작은 문제에 대한 함수로 반복해서 대치해 나가는 해법
    - 추정후 증명 : 결론을 추정하고 수학적 귀납법을 이용해 증명하는 방법
    - 마스터 정리 : 형식에 맞는 점화식의 복잡도를 바로 알 수 있음

- 반복대치 예제(2개)

```
T(n) = T(n-1) + n
T(n) = T(n-2) + (n-1) + n
T(n) = T(n-3) + (n-2) + (n-1) + n
.
T(n) = T(1) + 2 + 3 + ... + n = n(n+1) / 2 = Θ(n^2)
T(n) = 2T(n/2) + n
T(n) = 2(2T(n/4) + n/2) + n = 4T(n/4) + 2n
T(n) = 4(2T(n/8) + n/4) + 2n = 8T(n/8) + 3n
.
T(n) = 2^kT(n/2^k) + kn (n = 2^k 대입)
T(n) = nT(1) + kn = n + nlogn = O(nlogn)
```

- 추정후 증명 예제(1개)

```
Q. T(n) = 2T(n/2) + n의 복잡도는 T(n) = O(nlogn)이라고 추정하자. 그렇다면 충분히 큰 n에 대하여
T(n) <= cnlogn을 만족하는 양의 상수 c가 존재할 것이다.
A.경계조건 : T(2)<= c2log2를 만족하는 c가 존재한다.
T(n) = 2T(n/2) + n
		<= 2c(n/2)log(n/2) + n
     = cnlogn - cnlog2 + n
		 = cnlogn + (-clog2 + 1)n
		<= cnlogn (c가 1/log2 이상이면 되니 성립한다)
```

------

# ⭐5장⭐

- 이진 검색 트리 특징(3가지)
    - 각 노드의 키 값은 서로 달라야 한다
    - 최상위 레벨에 루트 노드가 있고, 각 노드는 최대 2개의 자식 노드를 가질 수 있다.
    - 임의의 노드는 왼쪽 자식의 키 값보다 크며, 오른쪽 자식의 키 값보다 작다.
- 이진 검색 트리 시간복잡도 : Θ(logn)(평균), Θ(n)(최악)(한쪽으로 치우쳐졌을 때)

```
treeSearch(t, x){ //t: 루트노드, x : 검색 키
	if(t == NIL or key[t] == x) then return t;
	if(x < key[t]) then return treeSearch(left[t], x);
	else return treeSearch(right[t], x);
}
treeInsert(t, x){
	if(t == NIL) then {x를 키로하는 노드를 t의 부모 밑에 매달고 return};
	if(x < key[t]) then return treeInsert(left[t], x);
	else return treeInsert(right[t], x);
}
treeDelete(t, x){
	if(x가 리프노드) then 그냥 r을 버린다;
	else if(x의 자식이 하나) then r의 부모가 r의 자식을 가르키게 한다;
	else r의 오른쪽 서브트리의 최소 원소를 삭제하고 삭제한 원소를 r의 자리에 놓는다;
}
```

- 레드 블랙 트리(균형잡힌 이진 검색 트리)(특징 4가지) : 이진 검색 트리가 Θ(n)이 나올 경우를 회피하기 위해 고려된 트리 → `시험 때 NIL 빼먹지 않도록 주의하자`

    - 루트는 블랙이다.
    - 리프노드(NIL)는 전부 블랙이다.
    - 노드가 레드면 그 노드의 자식은 전부 블랙이다.
    - 루트노드에서 임의의 리프노드에 이르기 까지의 블랙노드의 수는 모두 동일하다.

- 레드 블랙 트리 삽입 예제(1) : 20 15 3 12 5 11 6 40 25 18

    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5fbfa42a-4c6b-44bf-a954-f953fc7a90bf/Untitled.png)

- 레드 블랙 트리 삽입 예제(2) : 33 21 25 15 47 8 56 41 68

- 레드 블랙 트리의 왼쪽 서브트리와 오른쪽 서브 트리가 균형잡힌 이유 : 위 4가지 특성들로 인해서

- 레드 블랙 트리의 가능한 최대 깊이가 O(logn)인 이유 :

    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/34dcbb2e-9ff8-46e5-af1a-c6100f3b0f5b/Untitled.png)

------

# ⭐3장⭐

- `선택정렬`

```
selectionSort(A[], n){
	for last <- n downto 2{ //1번
		A[1...last] 중 가장 큰 수 A[k]를 찾는다; //2번
		A[k] <-> A[last]; //3번
	}
}
```

- 선택 정렬의 수행 시간은 모든 경우에 Θ(n^2)이다.
    - 1의 for loop는 n-1번 반복된다. 3번은 단순히 값을 교환하므로 상수 시간이 걸린다.
    - 2번은 1번에서 할당되는 last변수의 값 - 1만큼 씩 비교하기 때문에 (n-1) + (n-2) + … + 2 + 1 = n(n-1) / 2 횟수만큼 비교 되므로 Θ(n^2)의 시간복잡도를 가지게 된다.
- `버블정렬`

```
bubbleSort(A[], n){
	for last <- n downto 2 //1번
		for i <- 1 to last-1 //2번
			if(A[i] > A[i+1]) then A[i] <-> A[i+1]; //3번
}
```

- 버블 정렬의 수행시간은 모든 경우에 Θ(n^2)이다.
    - 1번의 for loop는 n-1번 반복된다. 3번은 단순히 값을 교환하므로 상수 시간이 걸린다.
    - 2번은 1번에서 할당되는 last변수의 값 - 1만큼 반복된다. 때문에 총 순환하는 횟수는 (n-1) + (n-2) + … + 2 + 1 = n(n-1) / 2 횟수만큼 반복되므로 Θ(n^2)의 시간복잡도를 가지게 된다.
- `삽입정렬`

```
insertionSort(A[], n){
	for i <- 2 to n{ //1번
		loc <- i-1;
		item <- A[i];
		while(loc >= 1 and item < A[loc]){//while전체 2번
			A[loc+1] <- A[loc];
			loc--;
		}
		A[loc+1] <- item;
	}
}
```

- 삽입정렬의 수행시간은 평균적으로 Θ(n^2), 최상일 때, Θ(n)이다.
    - 1번의 for loop는 n-1번 반복된다. 2번의 while loop는 최대 i-1번 반복되는데, 가장 운이 좋으면 item이 그자리 그대로 있게 되어 while문이 한번도 실행 되지 않는다.
    - 때문에 최악의 경우는 (n-1) + (n-2) + … + 2 + 1 = n(n-1) / 2 횟수만큼 반복되므로 Θ(n^2)이며, 최상의 경우는 1번의 for loop만 반복되므로 Θ(n)이다. 평균적으론 절반만 훑을 것 이므로 최악의 경우에 비해 절반 정도만 가지만 여전히 시간복잡도는 Θ(n^2)으로 표현이 된다.
    - 삽입정렬은 정렬이 이루어 졌을 경우에 혹은 거의 이루어졌을 경우에 while루프가 거의 실행되지 않음으로 매우 매력적인 알고리즘이 된다.
- `병합정렬`

```
mergeSort(A[n], p, r){
	if(p < r) then{
		q <- [(p+r)/2]; //[]->가우스 기호(ㄴ비슷함)(중간지점계산)
		mergeSort(A, p, q); //전반부 정렬
		mergeSort(A, q+1, r); //후반부 정렬
		merge(A, p, q, r); //병합
	}
}
merge(A[], p, q, r){
	정렬되어 있는 두 배열 A[p...q]와 A[q+1...r]을 합쳐
	정렬된 하나의 배열 A[p...r]을 만든다.
}
```

- 병합정렬의 수행시간은 최악, 평균, 최상일 때 전부 Θ(nlogn)이다.
    - 점근적 분석법은 반복 대치로 해서 외우자
- `퀵 정렬`

```
quickSort(A[], p, r){
	if(p<r) then{
		q = partition(A,p,r);
		quickSort(A,p,q-1);
		quickSort(A,q+1,r);
	}
}
partition(A[],p,r){
	배열 A[p...r]의 원소들을 A[r]기준으로 양쪽 재배치하고
	A[r]이 자리한 위치를 리턴한다;
}
```

- 퀵정렬의 수행시간은 평균적으로 Θ(nlogn), 최악일 때, Θ(n^2)이다.

퀵정렬에서 가장 핵심이 되는 부분은 어떻게 pivot을 선정하는지에 대한 부분이다. 퀵정렬의 최악의 경우의 시간복잡도는 O(N2) 이고 평균 복잡도는 𝛩(NlogN) 이기 때문에 피벗값을 잘못 선정하면 버블소트나 다름없는 성능을 보여준다.

피벗에 따라 시간복잡도가 극과극인 이유는 피봇을 통해 나누는 배열의 위치 때문이다. 만약 이미 정렬된 배열이나 역순으로 정렬된 배열이 있다고 하자. 이때 가장 처음값을 피벗으로 삼게되면 퀵소트 과정은 다음과 같이 진행된다.

1. `| 1 | 2 | 3 | 4 | 5 |` 이런 배열이 있을때, 1을 피벗으로 선택하게 되면,
2. 1을 기준으로 1보다 작은 값은 모두 왼쪽에 두고 큰 값은 모두 오른쪽에 두어야 하는데, 1보다 작은 값은 없기 때문에, 5부터 2까지 내려오면서 1과 비교하는 연산이 n-1번 수행된다.
3. 첫 분할이 끝나고 나면 다음 피벗은 2로 지정이 될텐데 이 때 역시 비교하는 연산이 n-1번 수행된다.
4. 따라서 피벗이 n에 도달할때까지 비교연산이 계속 진행되기 때문에 시간 복잡도는 `n-1 * n` 이 되어서 O(N^2) 가 되게된다.

반면에 배열이 정확하게 혹은 거의 근사하게 반으로 계속 나뉘어진다면 배열의 요소 갯수로 만들어지는 완전이진트리의 높이인 log n 에 대해 비교연산이 n번 수행되므로 시간 복잡도는 𝛩(nlogn) 이 된다.

이런 문제점에도 불구하고 퀵소트가 여전히 빠른 알고리즘으로 인정받는 이유는 완전히 정렬된 배열에 대해 퀵정렬을 수행할 가능성이 매우 적기 때문이다.

------